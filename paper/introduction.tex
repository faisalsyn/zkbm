\section{Introduction}\label{sec:intro}

Coordination is essential for large-scale distributed applications. Handling the simplest operations in a distributed manner gave rise to unprecedented challenges. Different forms of coordination are used to handle a variety of tasks. Leader election and group membership is one way. Worrying about aspects of synchronization, concurrency, and distributed management is a huge burden on application developers. This is why many distributed coordinators were designed to be leveraged by those developers, such as ZooKeeper and Chubby. Other packages focus on one primitive, or aspect, of distributed coordination such as Amazon Simple Queue Service that focuses on queueing. 

Synchonization mechanisms come at a cost however and have been an emphasis of research for a long time. {ref PA papers} Numerous papers look into reducing latency locally on a single machine through good choice of primitives, reactive algorithms, heuristics such as lock elison or hardware support for transactional memory. A different branch of research looks into distributed synchonization through protocols like two phase commit or Paxos in a search for fault-tolerance and graceful behavior under high contention. While the first approach provides low latency solutions, it does not scale across a local network or geo distributed datacenters. The latter approach provides fault-tolerance and predictable performance but comes with substantial latency overhead in the first place. The current emphasis on strongly consistent geo distributed application aggravates this latency issue. For example, Google's recent F-1 database system shows substantially higher latencies than its predecessor and pushes complexity down to the client API to mask some of this additional cost.

Here is a need to reinvestigate synchronization protocols for large-scale distributed systems. In these systems communication latency depends on network round trip times and can reach hundreds of milliseconds. This dramatic difference to the conventional multiprocessor environment might carry with it new revelations on the community's prejudice on traditional synchronization protocols. Existing distributed coordination packages deliver basic coordination schemes to end users and typically provide proven "recipies" for building synchronization mechanisms. For example, ZooKeeper provides a simple API to manipulate hierarchically organized wait-free data objects, resembling a file system. These manipulations are guaranteed to be FIFO ordered which enables users to quickly create more complex synchronization primitives such as locks and queues. Although this ease of of use is an advantage upfront, the inefficient implementation or inappropriate use of these low level primitives turns into low overall application throughput. This in turn leads to the installation of complex hierarchical layers of servers and quorums in order to cope with latency and throughput bottlenecks. \note{ref?}

In this paper we take first steps towards improved synchronization in geo distributed settings and show that the performance of synchronization mechanisms heavily depends on the use-case. For our experiments we focus on providing mutual exclusion for a resource with varying latency between client-server and multiple servers as well as different levels of contention for that resource. We investigate two primitives, queues and locks with two different implementations each. The implementations, synchonous and asynchronous, are inspired by traditional distributed systems "recipies" as well as approaches taken in parallel hardware architectures. While our results show familiar results for advantages of locks under low contention and queues under high load, a varying range of round trip times shows non-obvious benefits of asynchronous implementations.

The rest of the paper is organized as follows. Section~\ref{sec:framework} describe the framework of our experiments and overview basic concepts and technologies used. A mathematical analysis is presented in Section~\ref{sec:analysis} where we provide a model of observed latency. Experimental results are then displayed in Section~\ref{sec:eval}. Finally, the paper concludes with a summary and future directions in Section~\ref{sec:conclusions}.